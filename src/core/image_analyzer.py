"""
å›¾åƒè´¨é‡åˆ†ææ¨¡å— - ä¸“é—¨æ£€æµ‹é©¬èµ›å…‹æ•ˆåº”å’Œå›¾åƒè´¨é‡å˜åŒ–
"""
import numpy as np
from scipy import ndimage, fft
from skimage import filters, feature, measure
import functools
import time
from typing import Dict, Any, Tuple, Optional


class ImageQualityAnalyzer:
    """å›¾åƒè´¨é‡åˆ†æå™¨ - æ£€æµ‹é©¬èµ›å…‹æ•ˆåº”ã€çº¹ç†å˜åŒ–ç­‰"""
    
    @staticmethod
    def analyze_image_quality(image: np.ndarray, name: str = "å›¾åƒ") -> Dict[str, Any]:
        """
        å…¨é¢åˆ†æå›¾åƒè´¨é‡ï¼Œç‰¹åˆ«å…³æ³¨é©¬èµ›å…‹æ•ˆåº”

        Args:
            image: è¾“å…¥å›¾åƒ
            name: å›¾åƒåç§°ï¼ˆç”¨äºæŠ¥å‘Šï¼‰

        Returns:
            Dict: åŒ…å«å„ç§è´¨é‡æŒ‡æ ‡çš„å­—å…¸
        """
        print(f"\nğŸ“Š {name}è´¨é‡åˆ†æ:")

        # å¤§å›¾åƒä¼˜åŒ–ï¼šå¦‚æœå›¾åƒå¤ªå¤§ï¼Œè¿›è¡Œä¸‹é‡‡æ ·åˆ†æ
        original_shape = image.shape
        total_pixels = image.size

        if total_pixels > 2000000:  # 2Måƒç´ ä»¥ä¸Šè¿›è¡Œä¸‹é‡‡æ ·
            print(f"   ğŸ”§ æ£€æµ‹åˆ°å¤§å›¾åƒ({total_pixels:,}åƒç´ )ï¼Œä½¿ç”¨ä¸‹é‡‡æ ·åˆ†æä»¥æé«˜é€Ÿåº¦...")
            # è®¡ç®—ä¸‹é‡‡æ ·æ¯”ä¾‹ï¼Œç›®æ ‡çº¦1Måƒç´ 
            scale = np.sqrt(1000000 / total_pixels)
            new_h = int(image.shape[0] * scale)
            new_w = int(image.shape[1] * scale)

            # ä½¿ç”¨OpenCVè¿›è¡Œé«˜è´¨é‡ä¸‹é‡‡æ ·
            from skimage import transform
            img_resized = transform.resize(image, (new_h, new_w), anti_aliasing=True, preserve_range=True)
            img = img_resized.astype(np.float32)
            print(f"      ä¸‹é‡‡æ ·: {original_shape} â†’ {img.shape}")
        else:
            # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°
            if image.dtype != np.float32:
                img = image.astype(np.float32)
            else:
                img = image.copy()

        # å½’ä¸€åŒ–åˆ°[0,1]ä»¥ä¾¿ç»Ÿä¸€åˆ†æ
        img_min, img_max = img.min(), img.max()
        if img_max > img_min:
            img_norm = (img - img_min) / (img_max - img_min)
        else:
            img_norm = img.copy()
        
        analysis = {
            'name': name,
            'shape': image.shape,
            'dtype': str(image.dtype),
            'range': (float(img_min), float(img_max)),
        }
        
        # 1. åŸºç¡€ç»Ÿè®¡
        analysis.update(ImageQualityAnalyzer._basic_statistics(img_norm))
        
        # 2. çº¹ç†å¤æ‚åº¦åˆ†æï¼ˆé©¬èµ›å…‹çš„å…³é”®æŒ‡æ ‡ï¼‰
        analysis.update(ImageQualityAnalyzer._texture_complexity(img_norm))
        
        # 3. è¾¹ç¼˜è´¨é‡åˆ†æ
        analysis.update(ImageQualityAnalyzer._edge_quality(img_norm))
        
        # 4. é¢‘åŸŸåˆ†æ
        analysis.update(ImageQualityAnalyzer._frequency_analysis(img_norm))
        
        # 5. ç©ºé—´ç›¸å…³æ€§åˆ†æï¼ˆæ£€æµ‹å—çŠ¶æ•ˆåº”ï¼‰
        analysis.update(ImageQualityAnalyzer._spatial_correlation(img_norm))
        
        # 6. é©¬èµ›å…‹æ•ˆåº”æ£€æµ‹
        analysis.update(ImageQualityAnalyzer._mosaic_detection(img_norm))
        
        # æ‰“å°å…³é”®æŒ‡æ ‡
        ImageQualityAnalyzer._print_key_metrics(analysis)

        return analysis

    @staticmethod
    def compare_analyses(original: Dict[str, Any], processed: Dict[str, Any]):
        """å¯¹æ¯”ä¸¤ä¸ªåˆ†æç»“æœ"""
        print(f"   ğŸ“ˆ å‡å€¼å˜åŒ–: {original['mean']:.3f} â†’ {processed['mean']:.3f} ({processed['mean']/original['mean']:.2f}x)")
        print(f"   ğŸ“Š æ ‡å‡†å·®å˜åŒ–: {original['std']:.3f} â†’ {processed['std']:.3f} ({processed['std']/original['std']:.2f}x)")
        print(f"   ğŸ¨ çº¹ç†å¤æ‚åº¦å˜åŒ–: {original['local_var_mean']:.4f} â†’ {processed['local_var_mean']:.4f} ({processed['local_var_mean']/original['local_var_mean']:.2f}x)")
        print(f"   ğŸ” è¾¹ç¼˜å¯†åº¦å˜åŒ–: {original['edge_density']:.4f} â†’ {processed['edge_density']:.4f} ({processed['edge_density']/original['edge_density']:.2f}x)")
        print(f"   ğŸ“Š é«˜é¢‘èƒ½é‡å˜åŒ–: {original['high_freq_energy']:.3f} â†’ {processed['high_freq_energy']:.3f} ({processed['high_freq_energy']/original['high_freq_energy']:.2f}x)")
        print(f"   ğŸš¨ é©¬èµ›å…‹æŒ‡æ•°å˜åŒ–: {original['mosaic_index']:.4f} â†’ {processed['mosaic_index']:.4f} ({processed['mosaic_index']/original['mosaic_index']:.2f}x)")

        # è´¨é‡è¯„ä¼°
        if processed['mosaic_index'] > original['mosaic_index'] * 2:
            print(f"   âš ï¸  è­¦å‘Šï¼šé©¬èµ›å…‹æ•ˆåº”æ˜¾è‘—å¢åŠ ï¼")
        if processed['texture_irregularity'] > original['texture_irregularity'] * 3:
            print(f"   âš ï¸  è­¦å‘Šï¼šçº¹ç†å˜å¾—å¼‚å¸¸ä¸è§„å¾‹ï¼")
        if processed['high_freq_energy'] > original['high_freq_energy'] * 5:
            print(f"   âš ï¸  è­¦å‘Šï¼šé«˜é¢‘å™ªå£°æ˜¾è‘—å¢åŠ ï¼")
    
    @staticmethod
    def _basic_statistics(img: np.ndarray) -> Dict[str, float]:
        """åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'mean': float(np.mean(img)),
            'std': float(np.std(img)),
            'entropy': float(measure.shannon_entropy(img)),
            'dynamic_range': float(img.max() - img.min()),
        }
    
    @staticmethod
    def _texture_complexity(img: np.ndarray) -> Dict[str, float]:
        """çº¹ç†å¤æ‚åº¦åˆ†æ - é©¬èµ›å…‹ä¼šå¯¼è‡´çº¹ç†å¼‚å¸¸å¤æ‚"""
        # å±€éƒ¨æ–¹å·®ï¼ˆçª—å£å¤§å°5x5ï¼‰
        local_var = ndimage.generic_filter(img, np.var, size=5)
        
        # æ¢¯åº¦å¹…å€¼
        gx = ndimage.sobel(img, axis=1)
        gy = ndimage.sobel(img, axis=0)
        grad_mag = np.sqrt(gx*gx + gy*gy)
        
        # çº¹ç†èƒ½é‡ï¼ˆç°åº¦å…±ç”ŸçŸ©é˜µçš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
        # è®¡ç®—æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„çº¹ç†å˜åŒ–
        h_diff = np.abs(np.diff(img, axis=1))
        v_diff = np.abs(np.diff(img, axis=0))
        
        return {
            'local_var_mean': float(np.mean(local_var)),
            'local_var_std': float(np.std(local_var)),
            'local_var_max': float(np.max(local_var)),
            'grad_mag_mean': float(np.mean(grad_mag)),
            'grad_mag_std': float(np.std(grad_mag)),
            'texture_energy_h': float(np.mean(h_diff)),
            'texture_energy_v': float(np.mean(v_diff)),
            'texture_uniformity': float(1.0 / (1.0 + np.std(local_var))),  # è¶Šå°è¶Šä¸å‡åŒ€
        }
    
    @staticmethod
    def _edge_quality(img: np.ndarray) -> Dict[str, float]:
        """è¾¹ç¼˜è´¨é‡åˆ†æ"""
        # Cannyè¾¹ç¼˜æ£€æµ‹
        edges = feature.canny(img, sigma=1.0)
        edge_density = np.sum(edges) / edges.size
        
        # è¾¹ç¼˜è¿ç»­æ€§ï¼ˆé€šè¿‡å½¢æ€å­¦æ“ä½œè¯„ä¼°ï¼‰
        from skimage import morphology
        edge_dilated = morphology.dilation(edges, morphology.disk(2))
        edge_continuity = np.sum(edge_dilated) / np.sum(edges) if np.sum(edges) > 0 else 0
        
        # æ¢¯åº¦æ–¹å‘ä¸€è‡´æ€§
        gx = ndimage.sobel(img, axis=1)
        gy = ndimage.sobel(img, axis=0)
        grad_angle = np.arctan2(gy, gx)
        
        # è®¡ç®—æ¢¯åº¦æ–¹å‘çš„å±€éƒ¨ä¸€è‡´æ€§
        angle_diff = np.abs(np.diff(grad_angle, axis=1))
        angle_consistency = 1.0 - np.mean(angle_diff) / np.pi
        
        return {
            'edge_density': float(edge_density),
            'edge_continuity': float(edge_continuity),
            'gradient_consistency': float(angle_consistency),
        }
    
    @staticmethod
    def _frequency_analysis(img: np.ndarray) -> Dict[str, float]:
        """é¢‘åŸŸåˆ†æ - é©¬èµ›å…‹ä¼šäº§ç”Ÿå¼‚å¸¸çš„é«˜é¢‘æˆåˆ†"""
        # å¯¹äºå¤§å›¾åƒï¼Œè¿›ä¸€æ­¥ä¸‹é‡‡æ ·ä»¥åŠ é€ŸFFT
        if img.size > 500000:  # 500Kåƒç´ ä»¥ä¸Šå†æ¬¡ä¸‹é‡‡æ ·
            from skimage import transform
            scale = np.sqrt(500000 / img.size)
            new_h = max(64, int(img.shape[0] * scale))  # æœ€å°64åƒç´ 
            new_w = max(64, int(img.shape[1] * scale))
            img_small = transform.resize(img, (new_h, new_w), anti_aliasing=True, preserve_range=True)
        else:
            img_small = img

        # 2D FFT
        f_transform = fft.fft2(img_small)
        f_shift = fft.fftshift(f_transform)
        magnitude = np.abs(f_shift)
        
        # è®¡ç®—é¢‘åŸŸèƒ½é‡åˆ†å¸ƒ
        h, w = img_small.shape
        center_h, center_w = h // 2, w // 2
        
        # åˆ›å»ºé¢‘ç‡æ©ç 
        y, x = np.ogrid[:h, :w]
        dist_from_center = np.sqrt((x - center_w)**2 + (y - center_h)**2)
        
        # ä½é¢‘ã€ä¸­é¢‘ã€é«˜é¢‘åŒºåŸŸ
        max_dist = min(center_h, center_w)
        low_freq_mask = dist_from_center <= max_dist * 0.1
        mid_freq_mask = (dist_from_center > max_dist * 0.1) & (dist_from_center <= max_dist * 0.5)
        high_freq_mask = dist_from_center > max_dist * 0.5
        
        total_energy = np.sum(magnitude**2)
        low_freq_energy = np.sum(magnitude[low_freq_mask]**2) / total_energy
        mid_freq_energy = np.sum(magnitude[mid_freq_mask]**2) / total_energy
        high_freq_energy = np.sum(magnitude[high_freq_mask]**2) / total_energy
        
        return {
            'low_freq_energy': float(low_freq_energy),
            'mid_freq_energy': float(mid_freq_energy),
            'high_freq_energy': float(high_freq_energy),
            'freq_energy_ratio': float(high_freq_energy / (low_freq_energy + 1e-8)),
        }
    
    @staticmethod
    def _spatial_correlation(img: np.ndarray) -> Dict[str, float]:
        """ç©ºé—´ç›¸å…³æ€§åˆ†æ - æ£€æµ‹å—çŠ¶æ•ˆåº”"""
        # æ°´å¹³å’Œå‚ç›´æ–¹å‘çš„è‡ªç›¸å…³
        h_corr = np.corrcoef(img[:-1].flatten(), img[1:].flatten())[0, 1]
        v_corr = np.corrcoef(img[:, :-1].flatten(), img[:, 1:].flatten())[0, 1]
        
        # å¯¹è§’çº¿ç›¸å…³æ€§
        d1_corr = np.corrcoef(img[:-1, :-1].flatten(), img[1:, 1:].flatten())[0, 1]
        d2_corr = np.corrcoef(img[:-1, 1:].flatten(), img[1:, :-1].flatten())[0, 1]
        
        # å±€éƒ¨ç›¸å…³æ€§å˜åŒ–ï¼ˆæ£€æµ‹å—çŠ¶æ•ˆåº”ï¼‰
        block_size = 8
        h_blocks = img.shape[0] // block_size
        w_blocks = img.shape[1] // block_size
        
        block_correlations = []
        for i in range(h_blocks - 1):
            for j in range(w_blocks - 1):
                block1 = img[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]
                block2 = img[(i+1)*block_size:(i+2)*block_size, j*block_size:(j+1)*block_size]
                if block1.size > 0 and block2.size > 0:
                    corr = np.corrcoef(block1.flatten(), block2.flatten())[0, 1]
                    if not np.isnan(corr):
                        block_correlations.append(corr)
        
        block_corr_std = np.std(block_correlations) if block_correlations else 0
        
        return {
            'horizontal_correlation': float(h_corr) if not np.isnan(h_corr) else 0.0,
            'vertical_correlation': float(v_corr) if not np.isnan(v_corr) else 0.0,
            'diagonal_correlation_1': float(d1_corr) if not np.isnan(d1_corr) else 0.0,
            'diagonal_correlation_2': float(d2_corr) if not np.isnan(d2_corr) else 0.0,
            'block_correlation_std': float(block_corr_std),
        }
    
    @staticmethod
    def _mosaic_detection(img: np.ndarray) -> Dict[str, float]:
        """é©¬èµ›å…‹æ•ˆåº”æ£€æµ‹"""
        # å—çŠ¶æ•ˆåº”æ£€æµ‹ï¼šè®¡ç®—8x8å—çš„å†…éƒ¨æ–¹å·®vså—é—´æ–¹å·®
        block_size = 8
        h_blocks = img.shape[0] // block_size
        w_blocks = img.shape[1] // block_size
        
        intra_block_vars = []  # å—å†…æ–¹å·®
        inter_block_vars = []  # å—é—´æ–¹å·®
        
        block_means = np.zeros((h_blocks, w_blocks))
        
        # è®¡ç®—æ¯ä¸ªå—çš„å‡å€¼å’Œå†…éƒ¨æ–¹å·®
        for i in range(h_blocks):
            for j in range(w_blocks):
                block = img[i*block_size:(i+1)*block_size, j*block_size:(j+1)*block_size]
                block_mean = np.mean(block)
                block_var = np.var(block)
                
                block_means[i, j] = block_mean
                intra_block_vars.append(block_var)
        
        # è®¡ç®—å—é—´æ–¹å·®
        if h_blocks > 1 and w_blocks > 1:
            inter_block_var = np.var(block_means)
        else:
            inter_block_var = 0
        
        avg_intra_var = np.mean(intra_block_vars)
        
        # é©¬èµ›å…‹æŒ‡æ•°ï¼šå—å†…æ–¹å·®é«˜ + å—é—´å˜åŒ–å¤§ = é©¬èµ›å…‹æ•ˆåº”
        mosaic_index = avg_intra_var * (1 + inter_block_var)
        
        # çº¹ç†ä¸è§„å¾‹æ€§ï¼šå±€éƒ¨æ–¹å·®çš„æ–¹å·®
        local_var = ndimage.generic_filter(img, np.var, size=3)
        texture_irregularity = np.var(local_var)
        
        return {
            'avg_intra_block_variance': float(avg_intra_var),
            'inter_block_variance': float(inter_block_var),
            'mosaic_index': float(mosaic_index),
            'texture_irregularity': float(texture_irregularity),
        }
    
    @staticmethod
    def _print_key_metrics(analysis: Dict[str, Any]):
        """æ‰“å°å…³é”®æŒ‡æ ‡"""
        print(f"   ğŸ“ˆ åŸºç¡€ç»Ÿè®¡: å‡å€¼={analysis['mean']:.3f}, æ ‡å‡†å·®={analysis['std']:.3f}, ç†µ={analysis['entropy']:.3f}")
        print(f"   ğŸ¨ çº¹ç†å¤æ‚åº¦: å±€éƒ¨æ–¹å·®å‡å€¼={analysis['local_var_mean']:.4f}, æ¢¯åº¦å¹…å€¼={analysis['grad_mag_mean']:.4f}")
        print(f"   ğŸ” è¾¹ç¼˜è´¨é‡: è¾¹ç¼˜å¯†åº¦={analysis['edge_density']:.4f}, æ¢¯åº¦ä¸€è‡´æ€§={analysis['gradient_consistency']:.3f}")
        print(f"   ğŸ“Š é¢‘åŸŸåˆ†æ: é«˜é¢‘èƒ½é‡å æ¯”={analysis['high_freq_energy']:.3f}, é¢‘ç‡æ¯”={analysis['freq_energy_ratio']:.2f}")
        print(f"   ğŸ§© ç©ºé—´ç›¸å…³æ€§: æ°´å¹³={analysis['horizontal_correlation']:.3f}, å‚ç›´={analysis['vertical_correlation']:.3f}")
        print(f"   ğŸš¨ é©¬èµ›å…‹æ£€æµ‹: é©¬èµ›å…‹æŒ‡æ•°={analysis['mosaic_index']:.4f}, çº¹ç†ä¸è§„å¾‹æ€§={analysis['texture_irregularity']:.4f}")
        
        # é©¬èµ›å…‹æ•ˆåº”è­¦å‘Š
        if analysis['mosaic_index'] > 0.01:  # é˜ˆå€¼éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            print(f"   âš ï¸  æ£€æµ‹åˆ°å¯èƒ½çš„é©¬èµ›å…‹æ•ˆåº”ï¼")
        if analysis['texture_irregularity'] > 0.005:
            print(f"   âš ï¸  æ£€æµ‹åˆ°çº¹ç†å¼‚å¸¸ä¸è§„å¾‹ï¼")


def image_analysis_decorator(func):
    """
    å›¾åƒåˆ†æè£…é¥°å™¨ - è‡ªåŠ¨åˆ†æå¤„ç†å‰åçš„å›¾åƒè´¨é‡
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        # å‡è®¾ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å›¾åƒæ•°æ®
        if len(args) > 0:
            input_image = args[0]
            
            # åˆ†æåŸå§‹å›¾åƒ
            print("\n" + "="*60)
            print("ğŸ” å›¾åƒè´¨é‡åˆ†æå¼€å§‹")
            print("="*60)
            
            original_analysis = ImageQualityAnalyzer.analyze_image_quality(
                input_image, "åŸå§‹å›¾åƒ"
            )
            
            # æ‰§è¡ŒåŸå§‹å‡½æ•°
            print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œ {func.__name__}...")
            start_time = time.time()
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # åˆ†æå¤„ç†åçš„å›¾åƒ
            if isinstance(result, np.ndarray):
                processed_analysis = ImageQualityAnalyzer.analyze_image_quality(
                    result, "å¤„ç†åå›¾åƒ"
                )
                
                # å¯¹æ¯”åˆ†æ
                print(f"\nğŸ“Š å¤„ç†å‰åå¯¹æ¯”åˆ†æ:")
                ImageQualityAnalyzer.compare_analyses(original_analysis, processed_analysis)
            
            print(f"\nâ±ï¸  æ€»å¤„ç†æ—¶é—´: {execution_time:.2f}ç§’")
            print("="*60)
            
            return result
        else:
            return func(*args, **kwargs)
    
    return wrapper



